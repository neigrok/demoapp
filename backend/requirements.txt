# settings
pydantic-settings==2.6.1

# model downloading
aiohttp==3.11.10

# model inference
llama_cpp_python==0.3.5 -i https://abetlen.github.io/llama-cpp-python/whl/cpu

# request handling
pydantic==2.10.3
fastapi==0.115.6
uvicorn==0.32.1
